# Deep-Learning-with-RNNs-Sequence-Models

## Abstract
This Jupyter Notebook is for applying Hugging Face to various transformer models -  its library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.

Transformers provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between Jax, PyTorch and TensorFlow.

In this notebook, it will implement fill-mask model to generate inputs and labels from texts, question answering model can be used for answering questions, summarization is to summarize a document or an article into a shorter text, text generation to create a coherent portion of text that is a continuation from the given contextm amd some classification model is for classifying the texts, tokens and features. The sentences similarity and translation are operating below as well.

With each transformer model, at least one example will be given in the loaded models to show its results meet our task requirements or not. 

